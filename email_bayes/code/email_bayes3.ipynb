{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import*\n",
    "path_train_ham=\"./data_set/hw1_data/train/ham/\"\n",
    "path_train_rest=\"./data_set/hw1_data/train/restham/\"\n",
    "path_train_spam=\"./data_set/hw1_data/train/spam/\"\n",
    "path_test_ham = \"./data_set/hw1_data/test/ham/\"\n",
    "path_test_spam = \"./data_set/hw1_data/test/spam/\"\n",
    "\n",
    "def email_spilt(Strings):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', Strings) #匹配去除符号\n",
    "    return[tok.lower() for tok in listOfTokens if ((len(tok) > 2)&(len(tok)<12))] #去除长度小于2大于12的词\n",
    "\n",
    "def word_choose(vocabSet):  #去除停用词\n",
    "    with open('word.txt', 'r') as fpr:  #调用停用词表\n",
    "        content = fpr.read().split('\\n')\n",
    "    wordSet = set(content)\n",
    "    vocabSet = set(vocabSet)\n",
    "    uselessSet = vocabSet & wordSet\n",
    "    usefulSet = vocabSet - uselessSet\n",
    "    return list(usefulSet)\n",
    "\n",
    "def createVocabList(dataSet): #创建词汇表\n",
    "    vocabSet = set([]) #创建一个空集\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #通过并集操作将数据传入set，并且这是一个无重复集合\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def get_email():\n",
    "    docList_train=[]; classList_train=[];docList_test=[];classList_test=[];lajiwords=[]#docList为词汇集 classList为标签集\n",
    "    for parent,dir_names,file_names in os.walk(path_train_ham):\n",
    "        for file_name in file_names:\n",
    "            docList_train.append(email_spilt(open(path_train_ham+file_name).read()))\n",
    "            classList_train.append(1)\n",
    "    for parent,dir_names,file_names in os.walk(path_train_spam):\n",
    "        for file_name in file_names:\n",
    "            docList_train.append(email_spilt(open(path_train_spam+file_name).read()))\n",
    "            lajiwords.append(email_spilt(open(path_train_spam+file_name).read()))\n",
    "            classList_train.append(0)\n",
    "    for parent,dir_names,file_names in os.walk(path_test_ham):\n",
    "        for file_name in file_names:\n",
    "            docList_test.append(email_spilt(open(path_test_ham+file_name).read()))\n",
    "            classList_test.append(1)\n",
    "    for parent,dir_names,file_names in os.walk(path_test_spam):\n",
    "        for file_name in file_names:\n",
    "            docList_test.append(email_spilt(open(path_test_spam+file_name).read()))\n",
    "            classList_test.append(0)\n",
    "    return docList_train,classList_train,docList_test,classList_test,lajiwords\n",
    "\n",
    "def train_bayes(trainMatrix,trainCategory): #trainMatrix文档矩阵 trainCategory标签向量\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs) #P(C1) P(C2)=1-P(C1)\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Denom=2.0 ; p1Denom=2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==1:\n",
    "            p1Num +=trainMatrix[i] #p1Num 词向量的向量相加\n",
    "            p1Denom += sum(trainMatrix[i]) #p1Denom  标签向量相加\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vest = log(p1Num/p1Denom) #P(w|1)\n",
    "    p0Vest = log(p0Num/p0Denom) #P(W|0)\n",
    "    return p0Vest,p1Vest,pAbusive\n",
    "\n",
    "def judge_email(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testing_bayes():\n",
    "    trainMat=[]\n",
    "    for postinDoc in docList_train:\n",
    "        trainMat.append(bagOfword(vocabList,postinDoc,lajiwords))\n",
    "    p0V,p1V,pAb=train_bayes(trainMat,classList_train)\n",
    "    TP=0;FP=0;FN=0;TN=0\n",
    "    list1=[]\n",
    "    for i in range(len(docList_test)):\n",
    "        thisDoc = array(bagOfword(vocabList,docList_test[i],list1))\n",
    "        myjudge=judge_email(thisDoc,p0V, p1V, pAb)\n",
    "        if(classList_test[i]==1):\n",
    "            if(myjudge==1):\n",
    "                TP+=1\n",
    "            if(myjudge==0):\n",
    "                FN+=1\n",
    "        if(classList_test[i]==0):\n",
    "            if(myjudge==1):\n",
    "                FP+=1\n",
    "            if(myjudge==0):\n",
    "                TN+=1\n",
    "    P = TP / float(TP+FP)\n",
    "    R = TP / float(TP+FN)\n",
    "    print (\"TP=%d,FN=%d,FP=%d,TN=%d,P=%f,R=%f\" %(TP,FN,FP,TN,P,R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfword(vocabSet,inputSet,lajiwords): #vocabSet词汇表，inputSet输入文档\n",
    "    returnVec=[0]*len(vocabSet) #创建0向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabSet:\n",
    "            returnVec[vocabSet.index(word)] += 1\n",
    "    for word in lajiwords:\n",
    "        if returnVec[lajiwords.index(word)]>300:\n",
    "            returnVec[lajiwords.index(word)]*=1000\n",
    "    return returnVec #返回词汇频率信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docList_train,classList_train,docList_test,classList_test,lajiwords=get_email()\n",
    "vocabList = createVocabList(docList_train)#词汇表vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=2964,FN=47,FP=16,TN=1108,P=0.994631,R=0.984391\n"
     ]
    }
   ],
   "source": [
    "testing_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfword(vocabSet,inputSet,lajiwords): #vocabSet词汇表，inputSet输入文档\n",
    "    returnVec=[0]*len(vocabSet) #创建0向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabSet:\n",
    "            returnVec[vocabSet.index(word)] += 1\n",
    "    for word in lajiwords:\n",
    "        if returnVec[lajiwords.index(word)]>300:\n",
    "            returnVec[lajiwords.index(word)]*=600\n",
    "    return returnVec #返回词汇频率信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=2964,FN=47,FP=16,TN=1108,P=0.994631,R=0.984391\n"
     ]
    }
   ],
   "source": [
    "testing_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
